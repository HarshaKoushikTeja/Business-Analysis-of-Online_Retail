{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427a0c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Online Retail Analytics – Final Comprehensive Analysis\n",
    "\n",
    "Business Theme: Inventory Optimization & Customer Retention Strategy  \n",
    "\n",
    "This notebook analyzes a real-world online retail dataset to:\n",
    "- Clean and explore transaction data\n",
    "- Visualize geographic and temporal sales patterns\n",
    "- Build RFM-based customer segments\n",
    "- Statistically test key business hypotheses (H1–H5)\n",
    "- Cluster customers into behavioral groups\n",
    "- Predict which customers are likely to return in the next 3 months '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad2cdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import datetime as dt\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score, accuracy_score, roc_curve\n",
    "import plotly.express as px\n",
    "import os\n",
    "\n",
    "\n",
    "DATA_PATH = \"Online_Retail.xlsx\"\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs('figures', exist_ok=True)\n",
    "# os.makedirs('results', exist_ok=True)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ONLINE RETAIL ANALYTICS - FINAL ANALYSIS\")\n",
    "print(\"Business Theme: Inventory Optimization & Customer Retention Strategy\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31933557",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''1. Data Loading and Cleaning\n",
    "\n",
    "We first load the Online Retail dataset, inspect basic data quality, remove duplicates, \n",
    "and create a clean **sales dataset** with only valid, positive-quantity transactions.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e7c85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SECTION 1: DATA LOADING AND CLEANING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load data\n",
    "try:\n",
    "    df = pd.read_excel(DATA_PATH)\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'Online_Retail.xlsx' not found. Please ensure file is in directory.\")\n",
    "    raise\n",
    "\n",
    "# --- Data Quality Report ---\n",
    "print(\"\\n--- Data Quality Report ---\")\n",
    "print(f\"Total rows: {len(df):,}\")\n",
    "print(f\"Missing CustomerID: {df['CustomerID'].isna().sum():,} ({df['CustomerID'].isna().sum()/len(df)*100:.2f}%)\")\n",
    "print(f\"Missing Description: {df['Description'].isna().sum():,}\")\n",
    "print(f\"Negative Quantity (returns): {(df['Quantity'] < 0).sum():,}\")\n",
    "print(f\"Unique products: {df['StockCode'].nunique():,}\")\n",
    "print(f\"Countries: {df['Country'].nunique()}\")\n",
    "\n",
    "# Data Cleaning\n",
    "df.drop_duplicates(inplace=True)\n",
    "df_clean = df.dropna(subset=['CustomerID'])\n",
    "\n",
    "# Create Sales Dataset (Positive quantities only) for General Analysis\n",
    "df_sales = df_clean[(df_clean['Quantity'] > 0) & (df_clean['UnitPrice'] > 0)].copy()\n",
    "\n",
    "# Feature Engineering\n",
    "df_sales['TotalAmount'] = df_sales['Quantity'] * df_sales['UnitPrice']\n",
    "df_sales['InvoiceDate'] = pd.to_datetime(df_sales['InvoiceDate'])\n",
    "df_sales['Date'] = df_sales['InvoiceDate'].dt.date\n",
    "df_sales['Month'] = df_sales['InvoiceDate'].dt.month\n",
    "df_sales['Year'] = df_sales['InvoiceDate'].dt.year\n",
    "df_sales['DayOfWeek'] = df_sales['InvoiceDate'].dt.day_name()\n",
    "\n",
    "print(f\"\\nFinal clean dataset for sales analysis: {df_sales.shape}\")\n",
    "df_sales.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3b79de",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''2. Enhanced Data Visualization\n",
    "\n",
    "We explore:\n",
    "- Geographic revenue distribution across countries\n",
    "- Temporal revenue patterns using daily sales and 30-day moving average'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cd2c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SECTION 2: ENHANCED DATA VISUALIZATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 2.1 Geographic Visualization\n",
    "country_sales = df_sales.groupby('Country').agg({\n",
    "    'TotalAmount': 'sum'\n",
    "}).reset_index().sort_values('TotalAmount', ascending=False)\n",
    "\n",
    "fig = px.choropleth(\n",
    "    country_sales,\n",
    "    locations='Country',\n",
    "    locationmode='country names',\n",
    "    color='TotalAmount',\n",
    "    title='Global Revenue Distribution',\n",
    "    color_continuous_scale='Viridis'\n",
    ")\n",
    "fig.show()\n",
    "fig.write_html('figures/geographic_revenue_map.html')\n",
    "print(\"✓ Geographic revenue map created\")\n",
    "\n",
    "# 2.2 Top 10 Countries by Revenue\n",
    "top_countries = country_sales.head(10)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=top_countries, x='TotalAmount', y='Country', palette='viridis')\n",
    "plt.title('Top 10 Countries by Total Revenue', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Revenue (£)')\n",
    "plt.ylabel('Country')\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/top10_countries_revenue.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(\"Top 10 revenue countries visualized\")\n",
    "\n",
    "# 2.3 Time Series Analysis\n",
    "daily_sales = df_sales.groupby('Date')['TotalAmount'].sum().reset_index()\n",
    "daily_sales.set_index('Date', inplace=True)\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(daily_sales.index, daily_sales['TotalAmount'], color='#2E86AB', label='Daily Revenue')\n",
    "plt.plot(\n",
    "    daily_sales.index,\n",
    "    daily_sales['TotalAmount'].rolling(window=30).mean(),\n",
    "    color='#C73E1D',\n",
    "    linewidth=2,\n",
    "    label='30-day MA'\n",
    ")\n",
    "plt.title('Daily Revenue & Moving Average', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/time_series_analysis.png', dpi=300)\n",
    "plt.show()\n",
    "print(\"Time series analysis created\")\n",
    "\n",
    "# 2.4 Monthly Revenue Trend\n",
    "monthly_sales = df_sales.groupby(['Year', 'Month'])['TotalAmount'].sum().reset_index()\n",
    "monthly_sales['YearMonth'] = monthly_sales['Year'].astype(str) + '-' + monthly_sales['Month'].astype(str)\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "sns.lineplot(data=monthly_sales, x='YearMonth', y='TotalAmount', marker='o')\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Monthly Revenue Trend (Seasonality Highlight)', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Revenue (£)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/monthly_revenue_trend.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(\"Monthly seasonal sales trend created\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c595009",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''3. RFM Analysis and Customer Segmentation\n",
    "\n",
    "We compute:\n",
    "- **Recency** (days since last purchase)  \n",
    "- **Frequency** (number of invoices)  \n",
    "- **Monetary** (total spend)\n",
    "\n",
    "Then we build an RFM score and map customers to segments like **Champions**, **Loyal**, **At Risk**, etc.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfb4835",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SECTION 3: RFM ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "latest_date = df_sales['InvoiceDate'].max() + dt.timedelta(days=1)\n",
    "rfm = df_sales.groupby('CustomerID').agg({\n",
    "    'InvoiceDate': lambda x: (latest_date - x.max()).days,\n",
    "    'InvoiceNo': 'nunique',\n",
    "    'TotalAmount': 'sum'\n",
    "}).reset_index()\n",
    "rfm.columns = ['CustomerID', 'Recency', 'Frequency', 'Monetary']\n",
    "\n",
    "# Segmentation Logic\n",
    "rfm['R_Score'] = pd.qcut(rfm['Recency'], q=4, labels=[4, 3, 2, 1])\n",
    "rfm['F_Score'] = pd.qcut(rfm['Frequency'].rank(method='first'), q=4, labels=[1, 2, 3, 4])\n",
    "rfm['M_Score'] = pd.qcut(rfm['Monetary'], q=4, labels=[1, 2, 3, 4])\n",
    "rfm['RFM_Sum'] = rfm['R_Score'].astype(int) + rfm['F_Score'].astype(int) + rfm['M_Score'].astype(int)\n",
    "\n",
    "def segment_customer(score):\n",
    "    if score >= 9:\n",
    "        return 'Champions'\n",
    "    elif score >= 8:\n",
    "        return 'Loyal Customers'\n",
    "    elif score >= 6:\n",
    "        return 'Potential Loyalists'\n",
    "    elif score >= 5:\n",
    "        return 'At Risk'\n",
    "    else:\n",
    "        return 'Hibernating/Lost'\n",
    "\n",
    "rfm['Segment'] = rfm['RFM_Sum'].apply(segment_customer)\n",
    "print(rfm['Segment'].value_counts())\n",
    "\n",
    "rfm.head()\n",
    "\n",
    "# 3.4 Segment Distribution Visualization\n",
    "plt.figure(figsize=(8, 8))\n",
    "rfm['Segment'].value_counts().plot(kind='pie', autopct='%1.1f%%', startangle=90, colors=sns.color_palette('husl'))\n",
    "plt.title('Customer Segment Distribution')\n",
    "plt.ylabel('')\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/rfm_segment_pie.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Customer segment distribution chart created\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad15d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''4. Statistical Hypothesis Testing (H1 – H5)\n",
    "\n",
    "We test five business hypotheses:\n",
    "\n",
    "- **H1:** High-frequency customers have higher monetary value  \n",
    "- **H2:** Repeat customers have higher average order value than single-purchase customers  \n",
    "- **H3:** Transaction amounts differ by day of week  \n",
    "- **H4:** International customers have higher order value than UK customers  \n",
    "- **H5:** Return rates differ significantly across countries  \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a983ce2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SECTION 4: STATISTICAL HYPOTHESIS TESTING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# --- H1: Frequency-Monetary Relationship ---\n",
    "print(\"\\n--- H1: High-frequency vs Low-frequency Monetary Value ---\")\n",
    "median_freq = rfm['Frequency'].median()\n",
    "high_freq = rfm[rfm['Frequency'] >= median_freq]['Monetary']\n",
    "low_freq = rfm[rfm['Frequency'] < median_freq]['Monetary']\n",
    "t_stat, p_val = stats.ttest_ind(high_freq, low_freq)\n",
    "print(f\"High Freq Mean: £{high_freq.mean():.2f} | Low Freq Mean: £{low_freq.mean():.2f}\")\n",
    "print(f\"T-statistic: {t_stat:.4f}, P-value: {p_val:.4e}\")\n",
    "print(f\"Result: {'REJECT' if p_val < 0.05 else 'FAIL TO REJECT'} null hypothesis\")\n",
    "\n",
    "# --- H2: Repeat vs Single Purchase Average Order Value (MATCHING REPORT) ---\n",
    "print(\"\\n--- H2: Repeat vs Single Purchase Average Order Value ---\")\n",
    "\n",
    "# Average value of line items per customer\n",
    "customer_stats = df_sales.groupby('CustomerID').agg({\n",
    "    'InvoiceNo': 'nunique',\n",
    "    'TotalAmount': 'mean'  # average line-item value (Quantity * UnitPrice)\n",
    "}).reset_index()\n",
    "\n",
    "customer_stats.columns = ['CustomerID', 'NumOrders', 'AvgLineValue']\n",
    "\n",
    "repeat_customers = customer_stats[customer_stats['NumOrders'] > 1]['AvgLineValue']\n",
    "single_customers = customer_stats[customer_stats['NumOrders'] == 1]['AvgLineValue']\n",
    "\n",
    "t_stat, p_val = stats.ttest_ind(repeat_customers, single_customers)\n",
    "print(f\"Repeat customers avg value: £{repeat_customers.mean():.2f} (n={len(repeat_customers):,} customers)\")\n",
    "print(f\"Single customers avg value: £{single_customers.mean():.2f} (n={len(single_customers):,} customers)\")\n",
    "print(f\"T-statistic: {t_stat:.4f}, P-value: {p_val:.4f}\")\n",
    "print(f\"Result: {'REJECT' if p_val < 0.05 else 'FAIL TO REJECT'} null hypothesis\")\n",
    "\n",
    "# --- H3: Weekly Transaction Patterns ---\n",
    "print(\"\\n--- H3: Weekly Transaction Patterns (ANOVA) ---\")\n",
    "present_days = df_sales['DayOfWeek'].unique()\n",
    "day_groups = []\n",
    "valid_days = []\n",
    "\n",
    "for d in present_days:\n",
    "    group_data = df_sales[df_sales['DayOfWeek'] == d]['TotalAmount']\n",
    "    if len(group_data) > 10:\n",
    "        day_groups.append(group_data)\n",
    "        valid_days.append(d)\n",
    "\n",
    "print(f\"Days included in ANOVA: {valid_days}\")\n",
    "\n",
    "if len(day_groups) >= 2:\n",
    "    f_stat, p_val = stats.f_oneway(*day_groups)\n",
    "    print(f\"F-statistic: {f_stat:.4f}, P-value: {p_val:.4e}\")\n",
    "    print(f\"Result: {'REJECT' if p_val < 0.05 else 'FAIL TO REJECT'} null hypothesis\")\n",
    "else:\n",
    "    print(\"Insufficient groups for ANOVA.\")\n",
    "\n",
    "# --- H4: Domestic vs International ---\n",
    "print(\"\\n--- H4: UK vs International Order Values ---\")\n",
    "uk_orders = df_sales[df_sales['Country'] == 'United Kingdom']['TotalAmount']\n",
    "intl_orders = df_sales[df_sales['Country'] != 'United Kingdom']['TotalAmount']\n",
    "t_stat, p_val = stats.ttest_ind(intl_orders, uk_orders, equal_var=False)\n",
    "print(f\"Intl Mean: £{intl_orders.mean():.2f} | UK Mean: £{uk_orders.mean():.2f}\")\n",
    "print(f\"T-statistic: {t_stat:.4f}, P-value: {p_val:.4e}\")\n",
    "print(f\"Result: {'REJECT' if p_val < 0.05 else 'FAIL TO REJECT'} null hypothesis\")\n",
    "\n",
    "# --- H5: Country-Level Return Rates ---\n",
    "print(\"\\n--- H5: Country-Level Return Rates (Invoice Level) ---\")\n",
    "df_h5 = df_clean.copy()\n",
    "df_h5['IsReturn'] = (df_h5['Quantity'] < 0)\n",
    "\n",
    "invoice_status = df_h5.groupby(['Country', 'InvoiceNo'])['IsReturn'].any().reset_index()\n",
    "country_cross = pd.crosstab(invoice_status['Country'], invoice_status['IsReturn'])\n",
    "\n",
    "if False in country_cross.columns:\n",
    "    country_cross.rename(columns={False: 'Normal'}, inplace=True)\n",
    "if True in country_cross.columns:\n",
    "    country_cross.rename(columns={True: 'Return'}, inplace=True)\n",
    "\n",
    "if 'Return' not in country_cross.columns:\n",
    "    country_cross['Return'] = 0\n",
    "if 'Normal' not in country_cross.columns:\n",
    "    country_cross['Normal'] = 0\n",
    "\n",
    "country_cross['Total'] = country_cross['Normal'] + country_cross['Return']\n",
    "country_valid = country_cross[country_cross['Total'] >= 50].copy()\n",
    "\n",
    "chi2, p_val, dof, expected = stats.chi2_contingency(country_valid[['Normal', 'Return']])\n",
    "print(f\"Countries analyzed: {len(country_valid)}\")\n",
    "print(f\"Chi2: {chi2:.4f}, P-value: {p_val:.4e}\")\n",
    "print(f\"Result: {'REJECT' if p_val < 0.05 else 'FAIL TO REJECT'} null hypothesis\")\n",
    "\n",
    "# 4.6 Return Rate Heatmap\n",
    "return_rates = (country_valid['Return'] / country_valid['Total']).reset_index()\n",
    "return_rates.columns = ['Country', 'ReturnRate']\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(return_rates.pivot_table(values='ReturnRate', index='Country'),\n",
    "            annot=True, cmap='Reds', fmt='.2f')\n",
    "plt.title('Return Rate by Country')\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/return_rate_heatmap.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Return rate heatmap created\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc07c7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''5. Customer Segmentation via Clustering\n",
    "\n",
    "We apply clustering on log-transformed RFM features and compare:\n",
    "- K-Means\n",
    "- Hierarchical clustering\n",
    "- Gaussian Mixture Models\n",
    "\n",
    "We also select an optimal **K** using the silhouette score.'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30330aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SECTION 5: CUSTOMER SEGMENTATION (CLUSTERING)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "rfm_features = rfm[['Recency', 'Frequency', 'Monetary']]\n",
    "rfm_log = np.log1p(rfm_features)\n",
    "scaler = StandardScaler()\n",
    "rfm_scaled = scaler.fit_transform(rfm_log)\n",
    "\n",
    "print(\"Determining Optimal K...\")\n",
    "silhouette_scores = []\n",
    "K_range = range(2, 6)\n",
    "\n",
    "for k in K_range:\n",
    "    km = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    labels = km.fit_predict(rfm_scaled)\n",
    "    score = silhouette_score(rfm_scaled, labels)\n",
    "    silhouette_scores.append(score)\n",
    "    print(f\"K={k}, Silhouette={score:.4f}\")\n",
    "\n",
    "optimal_k = K_range[np.argmax(silhouette_scores)]\n",
    "print(f\"\\nOptimal K based on Silhouette: {optimal_k}\")\n",
    "\n",
    "comparison_k = 2\n",
    "print(f\"\\n--- Clustering Model Comparison (K={comparison_k}) ---\")\n",
    "\n",
    "models = {\n",
    "    'K-Means': KMeans(n_clusters=comparison_k, random_state=42, n_init=10),\n",
    "    'Hierarchical': AgglomerativeClustering(n_clusters=comparison_k),\n",
    "    'Gaussian Mixture': GaussianMixture(n_components=comparison_k, random_state=42)\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    labels = model.fit_predict(rfm_scaled)\n",
    "    sil = silhouette_score(rfm_scaled, labels)\n",
    "    db = davies_bouldin_score(rfm_scaled, labels)\n",
    "    print(f\"{name}: Silhouette={sil:.4f}, Davies-Bouldin={db:.4f}\")\n",
    "\n",
    "kmeans_final = KMeans(n_clusters=comparison_k, random_state=42, n_init=10)\n",
    "rfm['Cluster'] = kmeans_final.fit_predict(rfm_scaled)\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# 2D view: Recency vs Monetary\n",
    "plt.subplot(1, 2, 1)\n",
    "scatter = plt.scatter(\n",
    "    rfm_log['Recency'],\n",
    "    rfm_log['Monetary'],\n",
    "    c=rfm['Cluster'],\n",
    "    cmap='viridis',\n",
    "    alpha=0.6\n",
    ")\n",
    "plt.xlabel('Log Recency')\n",
    "plt.ylabel('Log Monetary')\n",
    "plt.title(f'Customer Clusters (K={comparison_k})')\n",
    "plt.colorbar(scatter, label='Cluster')\n",
    "\n",
    "# Normalized cluster centers\n",
    "plt.subplot(1, 2, 2)\n",
    "cluster_avg = rfm.groupby('Cluster')[['Recency', 'Frequency', 'Monetary']].mean()\n",
    "cluster_avg_norm = (cluster_avg - cluster_avg.min()) / (cluster_avg.max() - cluster_avg.min())\n",
    "cluster_avg_norm.plot(kind='bar', ax=plt.gca())\n",
    "plt.title('Normalized Cluster Centers')\n",
    "plt.xlabel('Cluster')\n",
    "plt.xticks(rotation=0)\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/clustering_results.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Clustering visualization created (saved to figures/)\")\n",
    "\n",
    "# 5.4 3D Cluster Visualization\n",
    "fig = px.scatter_3d(\n",
    "    rfm_log,\n",
    "    x='Recency',\n",
    "    y='Frequency',\n",
    "    z='Monetary',\n",
    "    color=rfm['Cluster'],\n",
    "    title='3D Customer Clusters (RFM Space)'\n",
    ")\n",
    "fig.write_html(\"figures/clusters_3d.html\")\n",
    "fig.show()\n",
    "\n",
    "print(\"✓ 3D cluster visualization created\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b077b544",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''6. Predictive Modeling – Churn / Retention Prediction\n",
    "\n",
    "We use a **time-based split**:\n",
    "- Train on all behavior before a cutoff (~90 days before the last date)\n",
    "- Predict whether a customer will **return in the next 3 months**\n",
    "\n",
    "Models compared:\n",
    "- Logistic Regression  \n",
    "- Decision Tree  \n",
    "- Random Forest  \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5e0686",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SECTION 6: PREDICTIVE MODELING - CHURN PREDICTION (CORRECTED)\")\n",
    "print(\"Methodology: Time-Series Split (Holdout Method)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1. Define Cutoff\n",
    "max_date = df_sales['InvoiceDate'].max()\n",
    "cutoff_date = max_date - pd.Timedelta(days=90)\n",
    "print(f\"Cutoff Date: {cutoff_date}\")\n",
    "\n",
    "# 2. Split Data\n",
    "train_data = df_sales[df_sales['InvoiceDate'] < cutoff_date].copy()\n",
    "test_data = df_sales[df_sales['InvoiceDate'] >= cutoff_date].copy()\n",
    "\n",
    "# 3. Feature Engineering\n",
    "X = train_data.groupby('CustomerID').agg({\n",
    "    'TotalAmount': 'sum',\n",
    "    'InvoiceNo': 'nunique',\n",
    "    'Quantity': 'sum',\n",
    "    'InvoiceDate': 'max'\n",
    "}).reset_index()\n",
    "\n",
    "X.columns = ['CustomerID', 'TotalSpent', 'Frequency', 'TotalQuantity', 'LastPurchaseDate']\n",
    "\n",
    "first_purchase = train_data.groupby('CustomerID')['InvoiceDate'].min().reset_index()\n",
    "first_purchase.columns = ['CustomerID', 'FirstPurchaseDate']\n",
    "X = X.merge(first_purchase, on='CustomerID')\n",
    "\n",
    "X['DaysSinceFirst'] = (cutoff_date - X['FirstPurchaseDate']).dt.days\n",
    "X['Recency'] = (cutoff_date - X['LastPurchaseDate']).dt.days\n",
    "X['AvgOrderValue'] = X['TotalSpent'] / X['Frequency']\n",
    "\n",
    "# 4. Target Variable\n",
    "future_customers = test_data['CustomerID'].unique()\n",
    "X['WillReturn'] = X['CustomerID'].isin(future_customers).astype(int)\n",
    "\n",
    "# 5. Prepare Data\n",
    "feature_cols = ['TotalSpent', 'AvgOrderValue', 'TotalQuantity', 'DaysSinceFirst', 'Recency', 'Frequency']\n",
    "X_model = X[feature_cols].fillna(0)\n",
    "y_model = X['WillReturn']\n",
    "\n",
    "scaler_pred = StandardScaler()\n",
    "X_scaled = scaler_pred.fit_transform(X_model)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y_model, test_size=0.25, random_state=42, stratify=y_model\n",
    ")\n",
    "\n",
    "# 6. Models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42),\n",
    "    'Decision Tree': DecisionTreeClassifier(max_depth=5, random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "}\n",
    "\n",
    "print(\"\\n--- Model Performance (with Cross-Validation) ---\")\n",
    "for name, model in models.items():\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='roc_auc')\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_prob = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_prob)\n",
    "    \n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  CV ROC-AUC: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})\")\n",
    "    print(f\"  Test Accuracy: {acc:.4f}\")\n",
    "    print(f\"  Test ROC-AUC: {auc:.4f}\")\n",
    "\n",
    "# 7. Classification Report & Feature Importance (Random Forest)\n",
    "best_model = models['Random Forest']\n",
    "y_pred_rf = best_model.predict(X_test)\n",
    "\n",
    "print(\"\\n--- Classification Report (Random Forest) ---\")\n",
    "print(classification_report(y_test, y_pred_rf, target_names=['Churned', 'Retained']))\n",
    "\n",
    "importances = pd.DataFrame({\n",
    "    'Feature': feature_cols,\n",
    "    'Importance': best_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"\\n--- Feature Importance ---\")\n",
    "print(importances)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ANALYSIS COMPLETE - OUTPUTS GENERATED\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
